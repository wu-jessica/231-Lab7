{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 7: Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 100 MB of random binary data containing 100%, 90%, 80%, 70%, 60%, and 50% zeros\n",
    "percentages = np.arange(50, 110, 10)\n",
    "\n",
    "for percentage in percentages:\n",
    "    rand_data = np.random.choice([0, 1], size=8*1024*1024*100, replace=True, p=[percentage/100,(1-(percentage/100))])\n",
    "    rand_data = np.packbits(rand_data)\n",
    "    open('zeros_'+str(percentage)+'p', 'wb').write(rand_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate DNA and protein sequences 100 million letters long, with each letter of equal probability\n",
    "seq_size = 100000000\n",
    "nt_letters = ['A','C','G','T']\n",
    "\n",
    "nt_seq = np.random.choice(nt_letters, size=seq_size, replace=True)\n",
    "open('nt_seq.fa', 'w').write(''.join(nt_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Amino acid alphabet](http://www.afrimage.org/amino-acid-alphabet/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105003247"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prot_letters = ['A','R','N','D','C','Q',',E','G','H','I','L','K','M','F','P','S','T','W','Y','V']\n",
    "\n",
    "prot_seq = np.random.choice(prot_letters, size=seq_size, replace=True)\n",
    "open('prot_seq.fa', 'w').write(''.join(prot_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terminal commands (repeated for all uncompressed input files)\n",
    "    be131-12@meowth:~/jess/Lab7$ time gzip -k zeros_100p\n",
    "\n",
    "    real    0m0.733s\n",
    "    user    0m0.692s\n",
    "    sys     0m0.040s\n",
    "    be131-12@meowth:~/jess/Lab7$ time bzip2 -k zeros_100p\n",
    "\n",
    "    real    0m1.054s\n",
    "    user    0m0.986s\n",
    "    sys     0m0.069s\n",
    "    be131-12@meowth:~/jess/Lab7$ cp zeros_100p zeros_100p_pbzip\n",
    "be131-12@meowth:~/jess/Lab7$ time pbzip2 -k zeros_100p_pbzip\n",
    "\n",
    "    real    0m0.107s\n",
    "    user    0m1.968s\n",
    "    sys     0m0.120s\n",
    "    be131-12@meowth:~/jess/Lab7$ time ArithmeticCompress zeros_100p zeros_100p.art\n",
    "\n",
    "    real    0m14.956s\n",
    "    user    0m14.744s\n",
    "    sys     0m0.213s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time and Memory of 4 Different Compression Methods on Simulated Data\n",
    "\n",
    "|Sequence|Input file size|Compression method|Output file size|Runtime (real)|\n",
    "| ------ |:-------------:|-----------------:|---------------:|:------------:|\n",
    "|100% zeros|105 MB|gz<br>bz2<br>pbz2<br>art<br>|102 kB<br>113 B<br>5.62 kB<br>1.03 kB|0.733s<br>0.962s<br>0.005s<br>14.956s|\n",
    "|90% zeros|105 MB|gz<br>bz2<br>pbz2<br>art<br>|58.7 MB<br>61.2 MB<br>61.2 MB<br>49.2 MB|18.864s<br>10.646s<br>0.768s<br>28.939s|\n",
    "|80% zeros|105 MB|gz<br>bz2<br>pbz2<br>art<br>|81.2 MB<br>86.6 MB<br>86.7 MB<br>75.7 MB|13.361s<br>12.009s<br>0.938s<br>35.362s|\n",
    "|70% zeros|105 MB|gz<br>bz2<br>pbz2<br>art<br>|93.6 MB<br>99.8 MB<br>99.8 MB<br>92.4 MB|6.061s<br>13.856s<br>1.176s<br>39.249s|\n",
    "|60% zeros|105 MB|gz<br>bz2<br>pbz2<br>art<br>|102 MB<br>105 MB<br>105 MB<br>102 MB|4.291s<br>15.722s<br>1.287s<br>41.070s|\n",
    "|50% zeros|105 MB|gz<br>bz2<br>pbz2<br>art<br>|105 MB<br>105 MB<br>105 MB<br>105 MB|3.562s<br>16.668s<br>1.537s<br>41.434s|\n",
    "|Nucleotide|100 MB|gz<br>bz2<br>pbz2<br>art<br>|29.2 MB<br>27.3 MB<br>27.3 MB<br>25 MB|12.151s<br>9.472s<br>0.680s<br>21.319s|\n",
    "|Protein|105 MB|gz<br>bz2<br>pbz2<br>art<br>|61.9 MB<br>55.3 MB<br>55.3 MB<br>57.7 MB|5.965s<br>10.468s<br>0.800s<br>30.236s|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which algorithm achieves the best level of compression on each file type?**  \n",
    "Binary files  \n",
    "100%: bzip  \n",
    "90% + 80%: gzip  \n",
    "70%:  ArithmeticCompress  \n",
    "60%: tie between gzip and ArithmeticCompress  \n",
    "50%: tie between all algorithms  \n",
    "\n",
    "Sequence files  \n",
    "nucleotide: ArithmeticCompress  \n",
    "protein: tie between bzip and pbzip2  \n",
    "\n",
    "**Which algorithm is the fastest?**  \n",
    "pbzip2 was the fastest algorithm for all files.  \n",
    "\n",
    "**What is the difference between bzip2 and pbzip2? Do you expect one to be faster and why?**  \n",
    "According to the Linux manual page for pbzip:  \n",
    ">\"pbzip2 is a parallel implementation of the bzip2 block-sorting file compressor that uses pthreads and achieves near-linear speedup on SMP machines. The output of this version is fully compatible with bzip2 v1.0.2 or newer (ie: anything compressed with pbzip2 can be decompressed with bzip2).  \n",
    ">pbzip2 should work on any system that has a pthreads compatible C++ compiler (such as gcc). It has been tested on: Linux, Windows (cygwin), Solaris, Tru64/OSF1, HP-UX, and Irix.\"  \n",
    "\n",
    "So because pbzip2 uses essentially the same algorithm as bzip2 compression but uses multiple threads to implement bzip2 compression in parallel, it makes sense for pbzip2 to be faster.  \n",
    "\n",
    "**How does the level of compression change as the percentage of zeros increases? Why does this happen?**  \n",
    "Compression ratio = Uncompressed size/Compressed size  \n",
    "\n",
    "|% zeros|Input file size|Compression method|Output file size|Compression ratio|Mean comp. ratio|\n",
    "| ----- |:-------------:|-----------------:|---------------:|:---------------:|------------------------:|\n",
    "|100%|105 MB|gz<br>bz2<br>pbz2<br>art<br>|102 kB<br>113 B<br>5.62 kB<br>1.03 kB|1054<br>9.74 x 10<sup>5</sup><br>1.91 x 10<sup>4</sup><br>1.04 x 10<sup>5</sup>|2.75 x 10<sup>5</sup>|\n",
    "|90%|105 MB|gz<br>bz2<br>pbz2<br>art<br>|58.7 MB<br>61.2 MB<br>61.2 MB<br>49.2 MB|1.79<br>1.72<br>1.72<br>2.13|1.84|\n",
    "|80%|105 MB|gz<br>bz2<br>pbz2<br>art<br>|81.2 MB<br>86.6 MB<br>86.7 MB<br>75.7 MB|1.29<br>1.21<br>1.21<br>1.39|1.27|\n",
    "|70%|105 MB|gz<br>bz2<br>pbz2<br>art<br>|93.6 MB<br>99.8 MB<br>99.8 MB<br>92.4 MB|1.12<br>1.05<br>1.05<br>1.14|1.09|\n",
    "|60%|105 MB|gz<br>bz2<br>pbz2<br>art<br>|102 MB<br>105 MB<br>105 MB<br>102 MB|1.03<br>1.00<br>1.00<br>1.03|1.01|\n",
    "|50%|105 MB|gz<br>bz2<br>pbz2<br>art<br>|105 MB<br>105 MB<br>105 MB<br>105 MB|1.00<br>1.00<br>1.00<br>1.00|1.00|  \n",
    "\n",
    "As the percentage of zeros increases, the level of compression increases. This is because files constructed from probability distributions with increasing percentages of zeros have a smaller expected Shannon information (entropy) needed to encode any outcome from the probability distribution. This means each position in a file contains less Shannon information on average as the percentage of zeros increases, so that an \"ideal\" data compression algorithm would perform better on it (greater compression ratio.)  \n",
    "\n",
    "**What is the minimum number of bits required to store a single DNA base?**  \n",
    "S = log<sub>2</sub>4 = 2 bits  \n",
    "\n",
    "**What is the minimum number of bits required to store an amino acid letter?**  \n",
    "S = log<sub>2</sub>20 = 4.32 bits  \n",
    "\n",
    "**In your tests, how many bits did gzip and bzip2 actually require to store your random DNA and protein sequences?**  \n",
    "\n",
    "|sequence| gzip  | bzip2 |\n",
    "|------- |------:|------:|\n",
    "|  DNA   |29.2 MB|27.3 MB|\n",
    "|protein |61.9 MB|55.3 MB|\n",
    "\n",
    "Min bits in DNA sequence (\"ideal\") = 10 x 10<sup>6</sup> positions x 2 bits/position = 2.0 x 10<sup>8</sup> bits  \n",
    "Actual gzip performance = 29.2 MB x 1024 kB/MB x 1024 B/kB x 8 bits/byte = 2.45 x 10<sup>8</sup> bits\n",
    "Actual bzip2 performance = 27.3 MB x 1024 kB/MB x 1024 B/kB x 8 bits/byte = 2.20 x 10<sup>8</sup> bits  \n",
    "\n",
    "Min bits in protein sequence (\"ideal\") = 10 x 10<sup>6</sup> positions x 4.32 bits/position = 4.32 x 10<sup>8</sup> bits  \n",
    "Actual gzip performance = 61.9 MB x 1024 kB/MB x 1024 B/kB x 8 bits/byte = 5.19 x 10<sup>8</sup> bits\n",
    "Actual bzip2 performance = 55.3 MB x 1024 kB/MB x 1024 B/kB x 8 bits/byte = 4.64 x 10<sup>8</sup> bits  \n",
    "\n",
    "**Are gzip and bzip2 performing well on DNA and proteins?**  \n",
    "Both gzip and bzip2 perform fairly well on DNA and protein sequences because they are able to encode the information in the sequences using an amount of bits fairly close to the minimum bits needed to encode those sequences (achieved by an \"ideal\" data compression algorithm), as shown in the calculations above. In both the DNA and protein sequences, bzip2 achieved data compression closer to \"ideal\" compression compared to gzip.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressing real data\n",
    "Using what you’ve learned about querying biological databases, find the\n",
    "nucleic acid sequences of gp120 homologs from at least 10 different HIV isolates and concatenate them together into a single multi-FASTA.  \n",
    "\n",
    "**A priori, do you expect to achieve better or worse compression here than random data? Why?**  \n",
    "I expect to achieve worse compression than on the simulated random data. This is because the FASTA format contains extra information in the form of the headers for each sequence. In addition, I expect the nucleic acid sequences to be fairly uniformly randomly distributed, similar to in the simulated nucleotide sequences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the nucleic acid sequences of gp120 homologs from at least 10 different HIV isolates and\n",
    "# concatenate them together into a single multi-FASTA.\n",
    "\n",
    "from Bio import Entrez, SeqIO\n",
    "# Define Entrez arguments\n",
    "Entrez.email = 'jessica.wu@berkeley.edu'\n",
    "query = 'gp120'\n",
    "\n",
    "fa = open('gp120.fa', 'w')\n",
    "num_homologs = 10\n",
    "for n in range(num_homologs):\n",
    "    # Get sequence field from nucleotide database \n",
    "    nt_handle = Entrez.esearch(db='nucleotide', term=query, sort='relevance')\n",
    "    i = Entrez.read(nt_handle)['IdList'][n] \n",
    "    fasta_handle = Entrez.efetch(db='nucleotide', id=i, rettype='fasta', retmode='text')\n",
    "\n",
    "    # Parse FASTA\n",
    "    for record in SeqIO.parse(fasta_handle, \"fasta\"):\n",
    "        fa.write('>' + record.description)\n",
    "        fa.write(str(record.seq) + '\\n')\n",
    "\n",
    "fa.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compress the multi-FASTA using gzip, bzip2, and arithmetic coding\n",
    "#### Terminal commands:  \n",
    "    be131-12@meowth:~/jess/Lab7$ time gzip -k gp120.fa\n",
    "    \n",
    "    real    0m0.003s\n",
    "    user    0m0.000s\n",
    "    sys     0m0.003s\n",
    "    be131-12@meowth:~/jess/Lab7$ time bzip2 -k gp120.fa\n",
    "\n",
    "    real    0m0.006s\n",
    "    user    0m0.006s\n",
    "    sys     0m0.000s\n",
    "    be131-12@meowth:~/jess/Lab7$ cp gp120.fa gp120_pbzip2.fa\n",
    "be131-12@meowth:~/jess/Lab7$ time pbzip2 -k gp120_pbzip2.fa\n",
    "\n",
    "    real    0m0.008s\n",
    "    user    0m0.004s\n",
    "    sys     0m0.004s\n",
    "    be131-12@meowth:~/jess/Lab7$ time ArithmeticCompress gp120.fa gp120.fa.art\n",
    "\n",
    "    real    0m0.011s\n",
    "    user    0m0.010s\n",
    "    sys     0m0.001s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time and Memory of 4 Different Compression Methods on Real Data (HIV gp120 multi-FASTA)  \n",
    "Input file size: 7.24 kB\n",
    "\n",
    "|Compression Method|Output file size|Runtime (real)|\n",
    "|-----------------:|---------------:|:------------:|\n",
    "|       gzip       |     1.9 kB     |    0.003s    |\n",
    "|       bzip2      |     1.97 kB    |    0.006s    |\n",
    "|      pbzip2      |     1.97 kB    |    0.008s    |\n",
    "|ArithmeticCompress|     3.45 kB    |    0.011s    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does the compression ratio of this file compare to random data?**  \n",
    "Compression ratio = Uncompressed size/Compressed size  \n",
    "\n",
    "|  DNA Data  |Input file size|Compression method|Output file size|Compression ratio|Mean comp. ratio|\n",
    "| ---------- |-----------------:|---------------:|:---------------:|------------------------:|\n",
    "|   Random   |100 MB|gz<br>bz2<br>pbz2<br>art<br>|29.2 MB<br>27.3 MB<br>27.3 MB<br>25 MB|3.42<br>3.66<br>3.66<br>4.00|3.69|\n",
    "|    Real    |7.24 kB|gz<br>bz2<br>pbz2<br>art<br>|1.9 kB<br>1.97 kB<br>1.97 kB<br>3.45 kB|3.81<br>3.68<br>3.68<br>2.10|3.32|\n",
    "\n",
    "Contrary to what I hypothesized, the file with real data had a slightly better compression ratio than of the simulated random data. I think this is because the the nucleic acid sequences are not as uniformly randomly distributed as I thought they would be. In order for the real sequences to have better compression, they have to have a lower expected Shannon information content (entropy) than the simulated random nucleotide sequence.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating compression of 1000 terabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make some assumptions about the contents of the data at your biotech company. Most of the data, say 80%, is re-sequencing of genomes and plasmids that are very similar to each other. Another 10% might be protein sequences, and the last 10% are binary microscope images which we’ll assume follow the worst-case scenario of being completely random. \n",
    "\n",
    "Protein sequence compression performance:\n",
    "\n",
    "|Input file size|Compression method|Output file size|Compression ratio|Mean comp. ratio|\n",
    "| ------------- |-----------------:|---------------:|:---------------:|---------------:|\n",
    "|105 MB|gz<br>bz2<br>pbz2<br>art<br>|61.9 MB<br>55.3 MB<br>55.3 MB<br>57.7 MB|1.70<br>1.90<br>1.90<br>1.82|1.83|\n",
    "\n",
    "**Given the benchmarking data you obtained in this lab, which algorithm do you propose to use for each type of data? Provide an estimate for the fraction of space you can save using your compression scheme. How much of a bonus do you anticipate receiving this year?**  \n",
    "\n",
    "|Data type|Compression method|Compression ratio|Mean comp. ratio|\n",
    "| ------- |-----------------:|----------------:|---------------:|\n",
    "|Real DNA sequences|gz<br>bz2<br>pbz2<br>art<br>|3.81<br>3.68<br>3.68<br>2.10|3.32|\n",
    "|Protein sequences|gz<br>bz2<br>pbz2<br>art<br>|1.70<br>1.90<br>1.90<br>1.82|1.83|\n",
    "|Random binary|gz<br>bz2<br>pbz2<br>art<br>|1.00<br>1.00<br>1.00<br>1.00|1.00|  \n",
    "\n",
    "According to the benchmarking data, I would use gzip for the real DNA sequencing data and pbzip2 for the protein sequences (same compression ratio but faster than bzip2.) I wouldn't bother compressing the completely random binary data because essentially no compression is possible using any of the algorithms, and given the large amount of data (10% of 1000 TB = 100 TB), it would be a waste of time.\n",
    "\n",
    "#### Estimate\n",
    "weighted compression ratio = 0.8(3.81) + 0.1(1.90) + 0.1(1) = 3.34  \n",
    "compressed data = 1000 TB/3.34 = 299.4 TB  \n",
    "(1000 TB - 299.4 TB)/1000 TB x 100 = 70% data reduction  \n",
    "70% reduction x (\\$500/day/1% reduction) x (365 days/year) = $12.8 million bonus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
